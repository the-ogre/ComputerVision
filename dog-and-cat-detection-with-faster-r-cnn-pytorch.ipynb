{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dog and Cat Detection","metadata":{"id":"kdyHqSFvSPmC"}},{"cell_type":"markdown","source":"- Torchvision provides several pre-trained object detection models, such as faster r-cnn and SSD. \n- These models have been pre-trained on the MS COCO image dataset, which contains 91 unique classes such as person, bicycle and traffic light to name a few.\n- We will use faster r-cnn model. ","metadata":{"id":"DXLo1bBFG26O"}},{"cell_type":"markdown","source":"##### During training, the model expects the following inputs:\n- Images as a list of torch tensors.\n- Targets as a list of dictionaries containing the bounding box coordinates (x1, y1, x2, y2) as well as the labels.\n\n##### During inference, the model takes a list of tensors as input, and returns a list of dictionaries with the following keys:\n\n- boxes: the predicted bounding boxes of any detected objects,\n- labels: the predicted object labels,\n- scores: the prediction confidence scores which range from 0–100%.","metadata":{"id":"ZrMEj9zdHGsm"}},{"cell_type":"markdown","source":"#### Outline\n\n1. download and prepare the training images and annotations from Kaggle,\n2. build the Datasets and Dataloaders required for inputting data into a torchvision model,\n3. download a pre-trained faster r-cnn model, and modify it to detect only dogs and cats out of the original 91 classes,\n4. perform transfer learning on the model using the downloaded dataset","metadata":{"id":"0iel0wODHeNm"}},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport torch\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:33:17.694632Z","iopub.execute_input":"2022-12-07T17:33:17.695391Z","iopub.status.idle":"2022-12-07T17:33:21.690174Z","shell.execute_reply.started":"2022-12-07T17:33:17.695307Z","shell.execute_reply":"2022-12-07T17:33:21.689129Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport PIL","metadata":{"id":"18T0Ff52Gykc","execution":{"iopub.status.busy":"2022-12-07T17:35:00.169528Z","iopub.execute_input":"2022-12-07T17:35:00.170316Z","iopub.status.idle":"2022-12-07T17:35:00.175996Z","shell.execute_reply.started":"2022-12-07T17:35:00.170269Z","shell.execute_reply":"2022-12-07T17:35:00.174706Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"https://pypi.org/project/torch-snippets/\n- torch snippets does a lot of default importing for you\n- Whether it is numpy, pandas, matplotlib or the useful functions that are mentioned below Simply call","metadata":{}},{"cell_type":"code","source":"!pip install torch-snippets","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:35:47.460352Z","iopub.execute_input":"2022-12-07T17:35:47.460727Z","iopub.status.idle":"2022-12-07T17:36:04.300512Z","shell.execute_reply.started":"2022-12-07T17:35:47.460694Z","shell.execute_reply":"2022-12-07T17:36:04.299149Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting torch-snippets\n  Downloading torch_snippets-0.499.9-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (3.7)\nRequirement already satisfied: srsly in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (2.4.5)\nRequirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (7.33.0)\nRequirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (0.0)\nRequirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (0.20.7)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (4.1.1)\nRequirement already satisfied: fastcore in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (1.5.27)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (6.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (9.1.1)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (0.4.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (3.5.3)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (1.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (1.21.6)\nCollecting typing\n  Downloading typing-3.7.4.3.tar.gz (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (1.3.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (0.3.5.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (4.64.0)\nRequirement already satisfied: wasabi in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (0.10.1)\nRequirement already satisfied: jsonlines in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (1.2.0)\nRequirement already satisfied: altair in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (4.2.0)\nRequirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (0.18.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (12.6.0)\nCollecting xmltodict\n  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\nRequirement already satisfied: confection in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (0.0.3)\nRequirement already satisfied: catalogue in /opt/conda/lib/python3.7/site-packages (from torch-snippets) (2.0.8)\nCollecting loguru\n  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->torch-snippets) (2.19.3)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->torch-snippets) (4.5.4.60)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->torch-snippets) (1.8.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->torch-snippets) (1.7.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->torch-snippets) (1.15.0)\nRequirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->torch-snippets) (0.19.3)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.7/site-packages (from altair->torch-snippets) (0.11.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from altair->torch-snippets) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.7/site-packages (from altair->torch-snippets) (4.6.1)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from altair->torch-snippets) (0.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-snippets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-snippets) (2022.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue->torch-snippets) (3.8.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastcore->torch-snippets) (21.3)\nRequirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastcore->torch-snippets) (22.1.2)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (0.18.1)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (5.3.0)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (59.8.0)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (2.12.0)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (0.7.5)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (5.1.1)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (4.8.0)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (0.1.3)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (0.2.0)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->torch-snippets) (3.0.30)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->torch-snippets) (0.11.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->torch-snippets) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->torch-snippets) (4.33.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->torch-snippets) (1.4.3)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->torch-snippets) (2021.11.10)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->torch-snippets) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->torch-snippets) (1.0.1)\nRequirement already satisfied: Levenshtein==0.20.7 in /opt/conda/lib/python3.7/site-packages (from python-Levenshtein->torch-snippets) (0.20.7)\nRequirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from Levenshtein==0.20.7->python-Levenshtein->torch-snippets) (2.11.1)\nRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from rich->torch-snippets) (0.9.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sklearn->torch-snippets) (1.0.2)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->torch-snippets) (0.8.3)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0->altair->torch-snippets) (0.18.1)\nRequirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0->altair->torch-snippets) (5.8.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0->altair->torch-snippets) (21.4.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0->altair->torch-snippets) (4.13.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->torch-snippets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->torch-snippets) (0.2.5)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (2.5)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (2021.11.2)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->altair->torch-snippets) (2.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn->torch-snippets) (3.1.0)\nBuilding wheels for collected packages: typing\n  Building wheel for typing (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=aa0304199d2f8dedc5563fff117509e71579ba850bf33df35d45b4c504e8372d\n  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\nSuccessfully built typing\nInstalling collected packages: xmltodict, typing, loguru, torch-snippets\nSuccessfully installed loguru-0.6.0 torch-snippets-0.499.9 typing-3.7.4.3 xmltodict-0.13.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.current_device()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:37:52.718627Z","iopub.execute_input":"2022-12-07T17:37:52.719774Z","iopub.status.idle":"2022-12-07T17:37:52.847690Z","shell.execute_reply.started":"2022-12-07T17:37:52.719708Z","shell.execute_reply":"2022-12-07T17:37:52.846610Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.device(0)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:38:07.457332Z","iopub.execute_input":"2022-12-07T17:38:07.457712Z","iopub.status.idle":"2022-12-07T17:38:07.464755Z","shell.execute_reply.started":"2022-12-07T17:38:07.457682Z","shell.execute_reply":"2022-12-07T17:38:07.463468Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<torch.cuda.device at 0x7facdf9cd050>"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:38:18.740534Z","iopub.execute_input":"2022-12-07T17:38:18.740954Z","iopub.status.idle":"2022-12-07T17:38:18.747272Z","shell.execute_reply.started":"2022-12-07T17:38:18.740919Z","shell.execute_reply":"2022-12-07T17:38:18.746152Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:38:27.303618Z","iopub.execute_input":"2022-12-07T17:38:27.304295Z","iopub.status.idle":"2022-12-07T17:38:27.310683Z","shell.execute_reply.started":"2022-12-07T17:38:27.304260Z","shell.execute_reply":"2022-12-07T17:38:27.309723Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'Tesla T4'"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:38:38.830657Z","iopub.execute_input":"2022-12-07T17:38:38.831114Z","iopub.status.idle":"2022-12-07T17:38:38.838920Z","shell.execute_reply.started":"2022-12-07T17:38:38.831079Z","shell.execute_reply":"2022-12-07T17:38:38.837649Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:39:24.850939Z","iopub.execute_input":"2022-12-07T17:39:24.851691Z","iopub.status.idle":"2022-12-07T17:39:24.863932Z","shell.execute_reply.started":"2022-12-07T17:39:24.851652Z","shell.execute_reply":"2022-12-07T17:39:24.862809Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Using device: cuda\n\nTesla T4\nMemory Usage:\nAllocated: 0.0 GB\nCached:    0.0 GB\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:39:44.407803Z","iopub.execute_input":"2022-12-07T17:39:44.408824Z","iopub.status.idle":"2022-12-07T17:39:45.578179Z","shell.execute_reply.started":"2022-12-07T17:39:44.408751Z","shell.execute_reply":"2022-12-07T17:39:45.576827Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Wed Dec  7 17:39:45 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   33C    P8     8W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To download and use kaggle data within Google Colab:\n1. Go to your account, Scroll to API section and Click Expire API Token to remove previous tokens\n2. Click on Create New API Token - It will download kaggle.json file on your machine.\n3. Go to your Google Colab project file and run the following commands:","metadata":{"id":"0jOtoehqIcMA"}},{"cell_type":"code","source":"!pip install -q kaggle","metadata":{"id":"u5_4ek9-Iw7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import files\nfiles.upload()         # expire any previous token(s) and upload recreated token","metadata":{"id":"t9m38BMFIt8v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" removes any file and delete .kaggle directory, move the uploaded token to a newly created directory and finishes off.","metadata":{"id":"-wFiG4CHI_s_"}},{"cell_type":"code","source":"#removes any file and delete .kaggle directory\n!rm -r ~/.kaggle\n\n#Choose the kaggle.json file that you downloaded\n#Make directory named kaggle and copy kaggle.json file there.\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n\n# Change the permissions of the file.\n!chmod 600 ~/.kaggle/kaggle.json\n\n#check if everything's okay by running this command.\n!kaggle datasets list","metadata":{"id":"agiWkX-YICcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle datasets download 'andrewmvd/dog-and-cat-detection'\n!unzip -q dog-and-cat-detection.zip","metadata":{"id":"U16Z_VxWKmUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT = \"/kaggle/input/dog-and-cat-detection/\"","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:39:57.894181Z","iopub.execute_input":"2022-12-07T17:39:57.894632Z","iopub.status.idle":"2022-12-07T17:39:57.902846Z","shell.execute_reply.started":"2022-12-07T17:39:57.894596Z","shell.execute_reply":"2022-12-07T17:39:57.901881Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"##### Decoding the Downloaded Annotation Files\n- We have two folders\n  - images: contains the .png images, \n  - annotations: contain the .xml annotation files.\n- the annotation file can be of many types. It will interesting to us xml file. Let's define a function to extract the bounding boxes and labels from the .xml files, we use the following function\n\n#### xml_to_dict(xml_path_\n- uses xml.etree.ElementTree to decode the .xml annotation files. \n- and returns the \n  - image size, \n  - object label and \n  - object bounding box coordinates \nas a dictionary.","metadata":{"id":"CHeQW65AKxgN"}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:40:02.261484Z","iopub.execute_input":"2022-12-07T17:40:02.262282Z","iopub.status.idle":"2022-12-07T17:40:02.267647Z","shell.execute_reply.started":"2022-12-07T17:40:02.262238Z","shell.execute_reply":"2022-12-07T17:40:02.266507Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def xml_to_dict(xml_path):\n    # Decode the .xml file\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    # Return the image size, object label and bounding box \n    # coordinates together with the filename as a dict.\n    return {\"filename\": xml_path,\n            \"image_width\": int(root.find(\"./size/width\").text),\n            \"image_height\": int(root.find(\"./size/height\").text),\n            \"image_channels\": int(root.find(\"./size/depth\").text),\n            \"label\": root.find(\"./object/name\").text,\n            \"x1\": int(root.find(\"./object/bndbox/xmin\").text),\n            \"y1\": int(root.find(\"./object/bndbox/ymin\").text),\n            \"x2\": int(root.find(\"./object/bndbox/xmax\").text),\n            \"y2\": int(root.find(\"./object/bndbox/ymax\").text)}","metadata":{"id":"8iScEEJQK43N","execution":{"iopub.status.busy":"2022-12-07T17:40:05.606986Z","iopub.execute_input":"2022-12-07T17:40:05.607358Z","iopub.status.idle":"2022-12-07T17:40:05.614989Z","shell.execute_reply.started":"2022-12-07T17:40:05.607326Z","shell.execute_reply":"2022-12-07T17:40:05.613819Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"##### Preparing the Datasets\n- Let's first create a CatDogDataset class \n- it inherits from torch.utils.data.Dataset \n- and loads the downloaded images and annotations into Python. \n\n##### CatDogDataset() \n- takes 2 arguments: \n  - root: a string containing the path to the data directory, and\n  - transforms: which contains torchvision image transformations.","metadata":{"id":"hZ_ldpOGLA4N"}},{"cell_type":"code","source":"# Convert human readable str label to int.\nlabel_dict = {\"dog\": 1, \"cat\" : 2}\n# Convert label int to human readable str.\nreverse_label_dict = {1: \"dog\", 2: \"cat\"}","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:40:10.261059Z","iopub.execute_input":"2022-12-07T17:40:10.261456Z","iopub.status.idle":"2022-12-07T17:40:10.266715Z","shell.execute_reply.started":"2022-12-07T17:40:10.261422Z","shell.execute_reply":"2022-12-07T17:40:10.265641Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class CatDogDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms = None):\n        \"\"\"\n        Inputs\n            root: str\n                Path to the data folder.\n            transforms: Compose or list\n                Torchvision image transformations.\n        \"\"\"\n        self.root = root\n        self.transforms = transforms\n        self.files = sorted(os.listdir(\"images\"))\n        for i in range(len(self.files)):\n            self.files[i] = self.files[i].split(\".\")[0]\n            self.label_dict = label_dict\n    def __getitem__(self, i):\n        # Load image from the hard disc.\n        img = PIL.Image.open(os.path.join(self.root, \n              \"images/\" + self.files[i] + \".png\")).convert(\"RGB\")\n        # Load annotation file from the hard disc.\n        ann = xml_to_dict(os.path.join(self.root, \n              \"annotations/\" + self.files[i] + \".xml\"))\n        # The target is given as a dict.\n        target = {}\n        target[\"boxes\"] = torch.as_tensor([[ann[\"x1\"], \n                                            ann[\"y1\"], \n                                            ann[\"x2\"], \n                                            ann[\"y2\"]]], \n                                   dtype = torch.float32)\n        target[\"labels\"]=torch.as_tensor([label_dict[ann[\"label\"]]],\n                         dtype = torch.int64)\n        target[\"image_id\"] = torch.as_tensor(i)\n        # Apply any transforms to the data if required.\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        return img, target\n    def __len__(self):\n        return len(self.files)","metadata":{"id":"m0bFUO7WK9yF","execution":{"iopub.status.busy":"2022-12-07T17:40:13.408384Z","iopub.execute_input":"2022-12-07T17:40:13.409382Z","iopub.status.idle":"2022-12-07T17:40:13.421634Z","shell.execute_reply.started":"2022-12-07T17:40:13.409333Z","shell.execute_reply":"2022-12-07T17:40:13.420643Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Image Transformations\n[https://pytorch.org/vision/main/transforms.html]\n\nTransforms are common image transformations available in the torchvision.transforms module. They can be chained together using Compose. Most transform classes have a function equivalent: functional transforms give fine-grained control over the transformations. This is useful if you have to build a more complex transformation pipeline (e.g. in the case of segmentation tasks).\n\nMost transformations accept both PIL images and tensor images, although some transformations are PIL-only and some are tensor-only. The Conversion Transforms may be used to convert to and from PIL images.\n\nThe transformations that accept tensor images also accept batches of tensor images. A Tensor Image is a tensor with (C, H, W) shape, where C is a number of channels, H and W are image height and width. A batch of Tensor Images is a tensor of (B, C, H, W) shape, where B is a number of images in the batch.\n\nThe expected range of the values of a tensor image is implicitly defined by the tensor dtype. Tensor images with a float dtype are expected to have values in [0, 1). Tensor images with an integer dtype are expected to have values in [0, MAX_DTYPE] where MAX_DTYPE is the largest value that can be represented in that dtype.\n\nRandomized transformations will apply the same transformation to all the images of a given batch, but they will produce different transformations across calls. For reproducible transformations across calls, you may use functional transforms.","metadata":{"id":"cY2VXtBEcZgN"}},{"cell_type":"markdown","source":"##### Image Transformations\n- Image transformations augment the training images to make the model more robust to noise or distortions.\n- Torchvision contains several such transformations, which can be composed together into a sequence using Compose. \n- Compose also allows for the composed transformations to be applied to images directly.","metadata":{"id":"eHyGTUMHLPWE"}},{"cell_type":"code","source":"import torchvision.transforms.functional as F\nimport torchvision.transforms.transforms as T","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:40:18.791866Z","iopub.execute_input":"2022-12-07T17:40:18.792247Z","iopub.status.idle":"2022-12-07T17:40:18.797462Z","shell.execute_reply.started":"2022-12-07T17:40:18.792215Z","shell.execute_reply":"2022-12-07T17:40:18.796022Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class Compose:\n    \"\"\"\n    Composes several torchvision image transforms \n    as a sequence of transformations.\n    Inputs\n        transforms: list\n            List of torchvision image transformations.\n    Returns\n        image: tensor\n        target: dict\n    \"\"\"\n    def __init__(self, transforms = []):\n        self.transforms = transforms\n    # __call__ sequentially performs the image transformations on\n    # the input image, and returns the augmented image.\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target","metadata":{"id":"1iBpR19NLMYd","execution":{"iopub.status.busy":"2022-12-07T17:40:21.735928Z","iopub.execute_input":"2022-12-07T17:40:21.736364Z","iopub.status.idle":"2022-12-07T17:40:21.745602Z","shell.execute_reply.started":"2022-12-07T17:40:21.736330Z","shell.execute_reply":"2022-12-07T17:40:21.744444Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"- we use two transformations: \n  - ToTensor converts a PIL image into a torch tensor (only for training set)\n  - RandomHorizontalFlip randomly flips an image horizontally. \n- ToTensor is applied to all images in order to convert the images into a format which can be input into the model, while RandomHorizontalFlip is applied only to the train image set.","metadata":{"id":"6z0wUKLuQduW"}},{"cell_type":"code","source":"class ToTensor(torch.nn.Module):\n    \"\"\"\n    Converts a PIL image into a torch tensor.\n    Inputs\n        image: PIL Image\n        target: dict\n    Returns\n        image: tensor\n        target: dict\n    \"\"\"\n    def forward(self, image, target = None):\n        image = F.pil_to_tensor(image)\n        image = F.convert_image_dtype(image)\n        return image, target\n\nclass RandomHorizontalFlip(T.RandomHorizontalFlip):\n    \"\"\"\n    Randomly flips an image horizontally.\n    Inputs\n        image: tensor\n        target: dict\n    Returns\n        image: tensor\n        target: dict\n    \"\"\"\n    def forward(self, image, target = None):\n        if torch.rand(1) < self.p:\n            image = F.hflip(image)\n            if target is not None:\n                width, _ = F.get_image_size(image)\n                target[\"boxes\"][:, [0, 2]] = width - \\\n                                     target[\"boxes\"][:, [2, 0]]\n        return image, target\n\n#get_transform is a helper function which composes several image transformations together for CatDogDataset using Compose. \n# Other transformations such as adding Gaussian blurring, image resizing or grayscale conversion can be added to get_transform \n# later on.\ndef get_transform(train):\n    \"\"\"\n    Transforms a PIL Image into a torch tensor, and performs\n    random horizontal flipping of the image if training a model.\n    Inputs\n        train: bool\n            Flag indicating whether model training will occur.\n    Returns\n        compose: Compose\n            Composition of image transforms.\n    \"\"\"\n    transforms = []\n    # ToTensor is applied to all images.\n    transforms.append(ToTensor())\n    # The following transforms are applied only to the train set.\n    if train == True:\n        transforms.append(RandomHorizontalFlip(0.5))\n        # Other transforms can be added here later on.\n    return Compose(transforms)","metadata":{"id":"xfJ7EnDnLg3k","execution":{"iopub.status.busy":"2022-12-07T17:40:25.685245Z","iopub.execute_input":"2022-12-07T17:40:25.686521Z","iopub.status.idle":"2022-12-07T17:40:25.696387Z","shell.execute_reply.started":"2022-12-07T17:40:25.686477Z","shell.execute_reply":"2022-12-07T17:40:25.695309Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"##### Train-Validation-Test Split\n- load and shuffle the data, and split the entire dataset into train, validation and test splits.","metadata":{"id":"n6aQCzlrQmZq"}},{"cell_type":"markdown","source":"- First of all, we load the data using CatDogDataset.","metadata":{"id":"NewWyOiuQ41-"}},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:43:06.629241Z","iopub.execute_input":"2022-12-07T17:43:06.629607Z","iopub.status.idle":"2022-12-07T17:43:06.636386Z","shell.execute_reply.started":"2022-12-07T17:43:06.629576Z","shell.execute_reply":"2022-12-07T17:43:06.635265Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"os.chdir(ROOT)\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:44:13.809540Z","iopub.execute_input":"2022-12-07T17:44:13.809953Z","iopub.status.idle":"2022-12-07T17:44:13.818501Z","shell.execute_reply.started":"2022-12-07T17:44:13.809909Z","shell.execute_reply":"2022-12-07T17:44:13.817464Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/dog-and-cat-detection'"},"metadata":{}}]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:44:24.271833Z","iopub.execute_input":"2022-12-07T17:44:24.272254Z","iopub.status.idle":"2022-12-07T17:44:24.283061Z","shell.execute_reply.started":"2022-12-07T17:44:24.272219Z","shell.execute_reply":"2022-12-07T17:44:24.281918Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"['annotations', 'images']"},"metadata":{}}]},{"cell_type":"code","source":"# Train dataset | Set train = True to apply the training image transforms.\ntrain_ds = CatDogDataset(\"./\", get_transform(train = True))\n\n# Validation dataset.\nval_ds = CatDogDataset(\"./\", get_transform(train = False))\n\n# Test dataset.\ntest_ds = CatDogDataset(\"./\", get_transform(train = False))","metadata":{"id":"WgtzP0PDQlEM","execution":{"iopub.status.busy":"2022-12-07T17:45:41.062867Z","iopub.execute_input":"2022-12-07T17:45:41.063234Z","iopub.status.idle":"2022-12-07T17:45:41.207251Z","shell.execute_reply.started":"2022-12-07T17:45:41.063204Z","shell.execute_reply":"2022-12-07T17:45:41.206211Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"- we randomly shuffle the data and split the data into the train-validation-test splits. \n- 80/20 train and test splits, \n- further split the train split into 80/20 train and validation sub-splits. \n- This results in a 64/16/20 split.","metadata":{"id":"ZTzIWPUuQ6SF"}},{"cell_type":"code","source":"# shuffle\nindices = torch.randperm(len(train_ds)).tolist()\n\n# Train dataset: 64% of the entire data\ntrain_ds = torch.utils.data.Subset(train_ds,\n           indices[:int(len(indices) * 0.64)])\n\n# Validation dataset: 16% of the entire data\nval_ds = torch.utils.data.Subset(val_ds, \n         indices[int(len(indices) * 0.64):int(len(indices) * 0.8)])\n\n# Test dataset: 20% of the entire data.\ntest_ds = torch.utils.data.Subset(test_ds, \n          indices[int(len(indices) * 0.8):])","metadata":{"id":"FpoG7zzbQ6tF","execution":{"iopub.status.busy":"2022-12-07T17:45:47.495071Z","iopub.execute_input":"2022-12-07T17:45:47.496277Z","iopub.status.idle":"2022-12-07T17:45:47.506539Z","shell.execute_reply.started":"2022-12-07T17:45:47.496229Z","shell.execute_reply":"2022-12-07T17:45:47.505191Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"##### Feed the data into the torchvision models\n- create DataLoaders to feed the data into the torchvision models.\n- We use a batch size of 16 images per batch. \n- Depending on your available GPU memory, you might need to change the batch size— smaller memory means smaller batch sizes!","metadata":{"id":"G7wpIqENRZXa"}},{"cell_type":"code","source":"BATCH_SIZE = 16","metadata":{"id":"zKnhCP2TZy4w","execution":{"iopub.status.busy":"2022-12-07T17:45:51.389619Z","iopub.execute_input":"2022-12-07T17:45:51.390588Z","iopub.status.idle":"2022-12-07T17:45:51.395918Z","shell.execute_reply.started":"2022-12-07T17:45:51.390551Z","shell.execute_reply":"2022-12-07T17:45:51.394763Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### TORCH.UTILS.DATA\nAt the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset, with support for\n  - map-style and iterable-style datasets,\n  - customizing data loading order,\n  - automatic batching,\n  - single- and multi-process data loading,\n  - automatic memory pinning.\n\nThese options are configured by the constructor arguments of a DataLoader, which has signature:\n\n[https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader]\n\n\nDataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n\nData loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n\nThe DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.","metadata":{"id":"5Pn9TKSXdgCl"}},{"cell_type":"markdown","source":"### Dataset Types\nThe most important argument of DataLoader constructor is dataset, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:\n  - map-style datasets,\n  - iterable-style datasets.\n\n##### Map-style datasets\nA map-style dataset is one that implements the __getitem__() and __len__() protocols, and represents a map from (possibly non-integral) indices/keys to data samples.\n\nFor example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk.\n\n\n##### Iterable-style datasets\nAn iterable-style dataset is an instance of a subclass of IterableDataset that implements the __iter__() protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.\n\nFor example, such a dataset, when called iter(dataset), could return a stream of data reading from a database, a remote server, or even logs generated in real time.","metadata":{"id":"W7q16A8Td4IN"}},{"cell_type":"code","source":"# Collate image-target pairs into a tuple.\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T17:45:58.079219Z","iopub.execute_input":"2022-12-07T17:45:58.079624Z","iopub.status.idle":"2022-12-07T17:45:58.084789Z","shell.execute_reply.started":"2022-12-07T17:45:58.079579Z","shell.execute_reply":"2022-12-07T17:45:58.083604Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Create the DataLoaders from the Datasets. \ntrain_dl = torch.utils.data.DataLoader(train_ds, \n                                 batch_size = BATCH_SIZE, \n                                 shuffle = True, \n                        collate_fn = collate_fn)\n\nval_dl = torch.utils.data.DataLoader(val_ds, \n                             batch_size = BATCH_SIZE, \n                            shuffle = False, \n                    collate_fn = collate_fn)\n\ntest_dl = torch.utils.data.DataLoader(test_ds, \n                               batch_size = BATCH_SIZE, \n                              shuffle = False, \n                      collate_fn = collate_fn)","metadata":{"id":"cUTakMt-Rb4Z","execution":{"iopub.status.busy":"2022-12-07T17:46:00.843895Z","iopub.execute_input":"2022-12-07T17:46:00.844277Z","iopub.status.idle":"2022-12-07T17:46:00.852017Z","shell.execute_reply.started":"2022-12-07T17:46:00.844244Z","shell.execute_reply":"2022-12-07T17:46:00.850951Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#### Download the Object Detection Model\n- The original faster r-cnn model locates 91 classes, and we have to make some modifications to the output layer of the model in order to focus on cats and dogs only.\n- Cats and dogs comprise 2 different classes. Additionally, the model also makes predictions for the background which is set to class 0 by default. Therefore we have to modify the model to locate 3 classes.","metadata":{"id":"zOw0KSBkRd7y"}},{"cell_type":"code","source":"NUM_CLASSES = 3","metadata":{"id":"e45K172IbNYn","execution":{"iopub.status.busy":"2022-12-07T17:46:05.043989Z","iopub.execute_input":"2022-12-07T17:46:05.044361Z","iopub.status.idle":"2022-12-07T17:46:05.049255Z","shell.execute_reply.started":"2022-12-07T17:46:05.044330Z","shell.execute_reply":"2022-12-07T17:46:05.048068Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"The torchvision.models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow.","metadata":{"id":"H79uXDO6a55W"}},{"cell_type":"markdown","source":"The Faster R-CNN model is based on the Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks paper.\t\n\n\nFaster R-CNN model with a ResNet-50-FPN backbone from the [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) paper. [https://pytorch.org/vision/main/models/faster_rcnn.html]","metadata":{"id":"2q44OvK-bpYm"}},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor","metadata":{"id":"cteFlQp5akyX","execution":{"iopub.status.busy":"2022-12-07T17:46:09.270578Z","iopub.execute_input":"2022-12-07T17:46:09.270945Z","iopub.status.idle":"2022-12-07T17:46:09.275500Z","shell.execute_reply.started":"2022-12-07T17:46:09.270912Z","shell.execute_reply":"2022-12-07T17:46:09.274500Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def get_object_detection_model(num_classes = NUM_CLASSES, \n                               feature_extraction = True):\n    \"\"\"\n    Inputs\n        num_classes: int\n            Number of classes to predict. includes the \n            background which is class 0 by definition\n        feature_extraction: bool\n            Flag indicating whether to freeze the pre-trained \n            weights. If set to True the pre-trained weights will be  \n            frozen and not be updated during.\n    Returns\n        model: FasterRCNN\n    \"\"\"\n    # Load the pretrained faster r-cnn model.\n    model = fasterrcnn_resnet50_fpn(pretrained = True)\n\n    # If True, the pre-trained weights will be frozen.\n    if feature_extraction == True:\n        for p in model.parameters():\n            p.requires_grad = False\n    \n    # Replace the original 91 class top layer with a new layer tailored for num_classes.\n    \n    # get number of input features for the classifier\n    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n    \n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats,\n                                                   num_classes)\n    return model","metadata":{"id":"AtTPJIhBRgkR","execution":{"iopub.status.busy":"2022-12-07T17:49:56.377895Z","iopub.execute_input":"2022-12-07T17:49:56.378648Z","iopub.status.idle":"2022-12-07T17:49:56.386139Z","shell.execute_reply.started":"2022-12-07T17:49:56.378605Z","shell.execute_reply":"2022-12-07T17:49:56.385066Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"##### Model Training Helper Functions\n- unbatching data from the Dataloaders\n- performing model training using back propagation.","metadata":{"id":"EVKBoiT2RjdJ"}},{"cell_type":"code","source":"def unbatch(batch, device):\n    \"\"\"\n    Unbatches a batch of data from the Dataloader.\n    Inputs\n        batch: tuple\n            Tuple containing a batch from the Dataloader.\n        device: str\n            Indicates which device (CPU/GPU) to use.\n    Returns\n        X: list\n            List of images.\n        y: list\n            List of dictionaries.\n    \"\"\"\n    X, y = batch\n    X = [x.to(device) for x in X]\n    y = [{k: v.to(device) for k, v in t.items()} for t in y]\n    return X, y\n\ndef train_batch(batch, model, optimizer, device):\n    \"\"\"\n    Uses back propagation to train a model.\n    Inputs\n        batch: tuple\n            Tuple containing a batch from the Dataloader.\n        model: torch model\n        optimizer: torch optimizer\n        device: str\n            Indicates which device (CPU/GPU) to use.\n    Returns\n        loss: float\n            Sum of the batch losses.\n        losses: dict\n            Dictionary containing the individual losses.\n    \"\"\"\n    model.train()\n    X, y = unbatch(batch, device = device)\n    optimizer.zero_grad()\n    losses = model(X, y)\n    loss = sum(loss for loss in losses.values())\n    loss.backward()\n    optimizer.step()\n    return loss, losses\n@torch.no_grad()\n\ndef validate_batch(batch, model, optimizer, device):\n    \"\"\"\n    Evaluates a model's loss value using validation data.\n    Inputs\n        batch: tuple\n            Tuple containing a batch from the Dataloader.\n        model: torch model\n        optimizer: torch optimizer\n        device: str\n            Indicates which device (CPU/GPU) to use.\n    Returns\n        loss: float\n            Sum of the batch losses.\n        losses: dict\n            Dictionary containing the individual losses.\n    \"\"\"\n    model.train()\n    X, y = unbatch(batch, device = device)\n    optimizer.zero_grad()\n    losses = model(X, y)\n    loss = sum(loss for loss in losses.values())\n    return loss, losses","metadata":{"id":"Xar23FLERm7p","execution":{"iopub.status.busy":"2022-12-07T17:50:01.164961Z","iopub.execute_input":"2022-12-07T17:50:01.165369Z","iopub.status.idle":"2022-12-07T17:50:01.176898Z","shell.execute_reply.started":"2022-12-07T17:50:01.165336Z","shell.execute_reply":"2022-12-07T17:50:01.175727Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"- Finally, we create a main driver function to actually train the downloaded faster r-cnn model using the helper functions above. \n- As PyTorch does not natively include any training history recorder, we use Report from the torch_snippets package to record our train and validation losses.","metadata":{"id":"OUsEagNpRqAS"}},{"cell_type":"code","source":"def train_fasterrcnn(model, \n                 optimizer, \n                  n_epochs, \n              train_loader, \n        test_loader = None, \n                log = None, \n               keys = None, \n            device = \"cpu\"):\n    \"\"\"\n    Trains a FasterRCNN model using train and validation \n    Dataloaders over n_epochs. \n    Returns a Report on the training and validation losses.\n    Inputs\n        model: FasterRCNN\n        optimizer: torch optimizer\n        n_epochs: int\n            Number of epochs to train.\n        train_loader: DataLoader\n        test_loader: DataLoader\n        log: Record\n            torch_snippet Record to record training progress.\n        keys: list\n            List of strs containing the FasterRCNN loss names.\n        device: str\n            Indicates which device (CPU/GPU) to use.\n    Returns\n        log: Record\n            torch_snippet Record containing the training records.\n    \"\"\"\n    if log is None:\n        log = Report(n_epochs)\n    if keys is None:\n        # FasterRCNN loss names.\n        keys = [\"loss_classifier\", \n                   \"loss_box_reg\", \n                \"loss_objectness\", \n               \"loss_rpn_box_reg\"]\n    \n    model.to(device)\n    \n    for epoch in range(n_epochs):\n        N = len(train_loader)\n        for ix, batch in enumerate(train_loader):\n            loss, losses = train_batch(batch, model, \n                                  optimizer, device)\n            # Record the current train loss.\n            pos = epoch + (ix + 1) / N\n            log.record(pos = pos, trn_loss = loss.item(), \n                       end = \"\\r\")\n        if test_loader is not None:\n            N = len(test_loader)\n            for ix, batch in enumerate(test_loader):\n                loss, losses = validate_batch(batch, model, \n                                         optimizer, device)\n                \n                # Record the current validation loss.\n                pos = epoch + (ix + 1) / N\n                log.record(pos = pos, val_loss = loss.item(), \n                           end = \"\\r\")\n    \n    log.report_avgs(epoch + 1)\n    return log","metadata":{"id":"oR_mXjABRruB","execution":{"iopub.status.busy":"2022-12-07T17:50:14.340661Z","iopub.execute_input":"2022-12-07T17:50:14.341078Z","iopub.status.idle":"2022-12-07T17:50:14.351815Z","shell.execute_reply.started":"2022-12-07T17:50:14.341042Z","shell.execute_reply":"2022-12-07T17:50:14.350789Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Training the Faster R-CNN Model\nNow that we have everything ready, we can finally start training the faster r-cnn model! We use the stochastic gradient descent optimizer and train the model over 1 epoch.","metadata":{"id":"n1vtnhmlRuvS"}},{"cell_type":"code","source":"EPOCHS = 15\nLEARNING_RATE = 0.001\nMOMENTUM = 0.9\nWEIGHT_DECAY = 0.0005","metadata":{"id":"u4jaCkQGe7zM","execution":{"iopub.status.busy":"2022-12-07T17:50:17.611103Z","iopub.execute_input":"2022-12-07T17:50:17.611983Z","iopub.status.idle":"2022-12-07T17:50:17.617528Z","shell.execute_reply.started":"2022-12-07T17:50:17.611943Z","shell.execute_reply":"2022-12-07T17:50:17.615548Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#create model\nmodel = get_object_detection_model(num_classes = NUM_CLASSES,   \n                        feature_extraction = False)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T18:02:19.878117Z","iopub.execute_input":"2022-12-07T18:02:19.878534Z","iopub.status.idle":"2022-12-07T18:02:27.165089Z","shell.execute_reply.started":"2022-12-07T18:02:19.878499Z","shell.execute_reply":"2022-12-07T18:02:27.163994Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef89273627e45e388e3a11c66ad8e1f"}},"metadata":{}}]},{"cell_type":"code","source":"# Use the stochastic gradient descent optimizer.\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, \n                        lr = LEARNING_RATE, \n                    momentum = MOMENTUM, \n             weight_decay = WEIGHT_DECAY)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T18:03:16.439878Z","iopub.execute_input":"2022-12-07T18:03:16.440252Z","iopub.status.idle":"2022-12-07T18:03:16.448199Z","shell.execute_reply.started":"2022-12-07T18:03:16.440224Z","shell.execute_reply":"2022-12-07T18:03:16.446975Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from torch_snippets import Report","metadata":{"execution":{"iopub.status.busy":"2022-12-07T18:07:35.507851Z","iopub.execute_input":"2022-12-07T18:07:35.508225Z","iopub.status.idle":"2022-12-07T18:07:43.742518Z","shell.execute_reply.started":"2022-12-07T18:07:35.508196Z","shell.execute_reply":"2022-12-07T18:07:43.741250Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"del train_ds\ndel val_ds\ndel test_ds","metadata":{"execution":{"iopub.status.busy":"2022-12-07T18:20:48.589804Z","iopub.execute_input":"2022-12-07T18:20:48.590843Z","iopub.status.idle":"2022-12-07T18:20:48.595893Z","shell.execute_reply.started":"2022-12-07T18:20:48.590798Z","shell.execute_reply":"2022-12-07T18:20:48.594583Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T18:22:02.401253Z","iopub.execute_input":"2022-12-07T18:22:02.401680Z","iopub.status.idle":"2022-12-07T18:22:02.406929Z","shell.execute_reply.started":"2022-12-07T18:22:02.401647Z","shell.execute_reply":"2022-12-07T18:22:02.405918Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"##### TORCH.CUDA.CURRENT_DEVICE\n- Read : [https://pytorch.org/docs/stable/generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info]\n- Returns the global free and total GPU memory occupied for a given device using cudaMemGetInfo.","metadata":{}},{"cell_type":"code","source":"torch.cuda.mem_get_info(device=torch.device(torch.cuda.current_device()))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T18:26:35.223339Z","iopub.execute_input":"2022-12-07T18:26:35.223710Z","iopub.status.idle":"2022-12-07T18:26:35.237517Z","shell.execute_reply.started":"2022-12-07T18:26:35.223676Z","shell.execute_reply":"2022-12-07T18:26:35.236484Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"(148635648, 15843721216)"},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-12-07T18:30:07.235478Z","iopub.execute_input":"2022-12-07T18:30:07.235893Z","iopub.status.idle":"2022-12-07T18:30:08.459532Z","shell.execute_reply.started":"2022-12-07T18:30:07.235858Z","shell.execute_reply":"2022-12-07T18:30:08.458140Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Wed Dec  7 18:30:08 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   59C    P0    26W /  70W |  14968MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Hugging Face Accelerate\nAccelerate is a lightweight framework designed for 4 things:\n\nConsolidating your code to be ran on CPU, TPU, and Multi-GPU\nA simplistic CLI launcher to do the above without having to remember complex commands\nA simplistic notebook launcher to do the above while inside a Jupyter Notebook\nAn interface to let you perform inference on huge models with small compute\nThat third item is what we'll focus on in this notebook\n\nKaggle comes preinstalled with accelerate already for you:\n\n","metadata":{}},{"cell_type":"markdown","source":"- upgrade it to enable the notebook_launcher through Github (or pypi once it is released and Kaggle hasn't updated it yet):","metadata":{}},{"cell_type":"markdown","source":"notebook_launcher\nThe notebook_launcher is a small function that can launch your code on a notebook on TPUs or multiple GPUs.\n\nTo use it you pass in the function, the arguments as a tuple, and the number of processes to train on.\n\n(See the basic tutorial to learn more)","metadata":{}},{"cell_type":"code","source":"# Train the model over 15 epochs.\nlog = train_fasterrcnn(model = model, \n               optimizer = optimizer, \n                        n_epochs = 1,\n             train_loader = train_dl, \n                test_loader = val_dl,\n             log = None, keys = None,\n                     device = device)","metadata":{"id":"Eo0JayOzRxwJ","execution":{"iopub.status.busy":"2022-12-07T18:42:38.579291Z","iopub.execute_input":"2022-12-07T18:42:38.579745Z","iopub.status.idle":"2022-12-07T18:42:39.331127Z","shell.execute_reply.started":"2022-12-07T18:42:38.579708Z","shell.execute_reply":"2022-12-07T18:42:39.327960Z"},"trusted":true},"execution_count":72,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3847164923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m              \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                      device = device)\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/1036081616.py\u001b[0m in \u001b[0;36mtrain_fasterrcnn\u001b[0;34m(model, optimizer, n_epochs, train_loader, test_loader, log, keys, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             loss, losses = train_batch(batch, model, \n\u001b[0;32m---> 45\u001b[0;31m                                   optimizer, device)\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Record the current train loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1352792728.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(batch, model, optimizer, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/backbone_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 14.76 GiB total capacity; 12.92 GiB already allocated; 441.75 MiB free; 13.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 14.76 GiB total capacity; 12.92 GiB already allocated; 441.75 MiB free; 13.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"markdown","source":"### Model Predictions\n- Now that we have a faster r-cnn model trained specifically to detect cats and dogs, we will make some predictions using images from the test set.","metadata":{"id":"ouCQat9YR1nb"}},{"cell_type":"markdown","source":"###### Helper functions\n- predict()\n- predict_batch()","metadata":{"id":"1Pj2ClFxfkKD"}},{"cell_type":"code","source":"@torch.no_grad() #when testing the model","metadata":{"id":"7E7gfPS0fhB8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_batch(batch, model, device):\n    \"\"\"\n    Gets the predictions for a batch of data.\n    Inputs\n        batch: tuple\n            Tuple containing a batch from the Dataloader.\n        model: torch model\n        device: str\n            Indicates which device (CPU/GPU) to use.\n    Returns\n        images: list\n            List of tensors of the images.\n        predictions: list\n            List of dicts containing the predictions for the \n            bounding boxes, labels and confidence scores.\n    \"\"\"\n    model.to(device)\n    model.eval()\n    X, _ = unbatch(batch, device = device)\n    predictions = model(X)\n    return [x.cpu() for x in X], predictions\n\ndef predict(model, data_loader, device = \"cpu\"): \n    \"\"\"\n    Gets the predictions for a batch of data.\n    Inputs\n        model: torch model\n        data_loader: torch Dataloader\n        device: str\n            Indicates which device (CPU/GPU) to use.\n    Returns\n        images: list\n            List of tensors of the images.\n        predictions: list\n            List of dicts containing the predictions for the \n            bounding boxes, labels and confidence scores.\n    \"\"\"\n    images = []\n    predictions = []\n    for i, batch in enumerate(data_loader):\n        X, p = predict_batch(batch, model, device)\n        images = images + X\n        predictions = predictions + p\n    \n    return images, predictions","metadata":{"id":"AZR8ElobRxqr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model outputs predictions as a list of dictionaries with the following keys:\n- boxes: bounding boxes of any detected objects,\n- scores: model prediction confidence scores,\n- labels: labels of any detected objects.\n\n\n###### decode_prediction()\n- decode_prediction takes decodes this dictionary, and filters out any predictions with a score lower than score_threshold\n- Additionally, non-maximum suppression is used to remove any overlapping bounding boxes using nms_iou_threshold.","metadata":{"id":"AWiQDF3mR4Uj"}},{"cell_type":"code","source":"def decode_prediction(prediction, \n                      score_threshold = 0.8, \n                      nms_iou_threshold = 0.2):\n    \"\"\"\n    Inputs\n        prediction: dict\n        score_threshold: float\n        nms_iou_threshold: float\n    Returns\n        prediction: tuple\n    \"\"\"\n    boxes = prediction[\"boxes\"]\n    scores = prediction[\"scores\"]\n    labels = prediction[\"labels\"]\n\n    # Remove any low-score predictions.\n    if score_threshold is not None:\n        want = scores > score_threshold\n        boxes = boxes[want]\n        scores = scores[want]\n        labels = labels[want]\n    # Remove any overlapping bounding boxes using NMS.\n    \n    if nms_iou_threshold is not None:\n        want = torchvision.ops.nms(boxes = boxes, scores = scores, \n                                iou_threshold = nms_iou_threshold)\n        boxes = boxes[want]\n        scores = scores[want]\n        labels = labels[want]\n    return (boxes.cpu().numpy(), \n            labels.cpu().numpy(), \n            scores.cpu().numpy())","metadata":{"id":"Gfz-Om-ER6vL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With these functions prepared, the model’s predictions on the test dataset can be obtained together with the images used.","metadata":{"id":"Ng5CSCb7R-Z6"}},{"cell_type":"code","source":"images, predictions = predict(model, test_dl, device)","metadata":{"id":"ZXDnNl1oR_fa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The predicted bounding box, labels and scores can be displayed together with the images to visualize the model’s outputs.","metadata":{"id":"n0zbZ-rYSCp6"}},{"cell_type":"code","source":"img_index = 0\nboxes, labels, scores = decode_prediction(predictions[img_index])\nfig, ax = plt.subplots(figsize = [5, 5])\nax.imshow(images[img_index].permute(1, 2, 0).numpy())\n\nfor i, b in enumerate(boxes):\n    rect = patches.Rectangle(b[:2].astype(int),\n                             (b[2] - b[0]).astype(int),\n                             (b[3] - b[1]).astype(int),\n                             linewidth = 1,\n                             edgecolor = \"r\",\n                             facecolor = \"none\")\n    ax.add_patch(rect)\n    ax.text(b[0].astype(int),\n            b[1].astype(int) - 5,\n            \"{} : {:.3f}\".format(reverse_label_dict[labels[i]],\n            scores[i]), color = \"r\")\nplt.show()","metadata":{"id":"GBc0OnrwSDk5"},"execution_count":null,"outputs":[]}]}